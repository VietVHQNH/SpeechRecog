{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "from multiprocessing.pool import Pool\n",
    "from multiprocessing import Manager\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, out_size = 10):\n",
    "        self.data_path = data_path\n",
    "        self.out_size = out_size\n",
    "        \n",
    "\n",
    "    def single_lstm(self, input_shape, output_bias = None):\n",
    "        if output_bias is not None:\n",
    "            output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape = input_shape))\n",
    "        model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (5,input_shape[1]), activation = 'relu'))\n",
    "        model.add(tf.keras.layers.Reshape((input_shape[0]-4,32)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.LSTM(64, return_sequences=True))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.LSTM(64))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layeres.Dropout(0.2))\n",
    "        model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', bias_initializer=output_bias))\n",
    "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[tf.keras.metrics.Recall(),\n",
    "                               tf.keras.metrics.Precision(),\n",
    "                               tfa.metrics.F1Score(num_classes=1, average='macro',threshold=0.5),\n",
    "                               'accuracy'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, data_path, out_size, speech_len, validation_split = 0.1):\n",
    "        self.data_path = data_path\n",
    "        self.out_size = out_size\n",
    "        self.speech_length = librosa.time_to_samples(speech_len)\n",
    "        self.data_list = glob.glob(os.path.join(self.data_path,\"CC_*\"))\n",
    "        random.shuffle(self.data_list)\n",
    "        self.train = self.data_list[:-int(len(self.data_list)*validation_split)]\n",
    "        self.valid = self.data_list[-int(len(self.data_list)*validation_split):]\n",
    "        print(len(self.train), len(self.valid))\n",
    "    def empty_sequence(self, n_part):\n",
    "        return [[0 for _ in range(self.speech_length)] for _ in range(n_part)]\n",
    "        \n",
    "    def train_generator(self, batch_size):\n",
    "        manager = Manager()\n",
    "        while True:\n",
    "            process = []\n",
    "            self.out = manager.list()\n",
    "            for _ in range(batch_size):\n",
    "                p = Process(target = self.load_files, args = (self.train,))\n",
    "                p.start()\n",
    "                process.append(p)\n",
    "            for p in process:\n",
    "                p.join()\n",
    "            out_data = []\n",
    "            out_labels = []\n",
    "            for data, label in self.out:\n",
    "                if data is not None and label is not None:\n",
    "                    out_data.append(data)\n",
    "                    out_labels.append(label)\n",
    "            if len(out_data) == 0 or len(out_labels) == 0:\n",
    "                continue\n",
    "            if len(np.array(out_data, dtype=object).shape)<4 or len(out_labels) == 0:\n",
    "                continue\n",
    "            yield np.array(out_data), np.array(out_labels)\n",
    "        \n",
    "    def valid_generator(self, batch_size):\n",
    "        manager = Manager()\n",
    "        while True:\n",
    "            process = []\n",
    "            self.out = manager.list()\n",
    "            for _ in range(batch_size):\n",
    "                p = Process(target = self.load_files, args = (self.valid,))\n",
    "                p.start()\n",
    "                process.append(p)\n",
    "            for p in process:\n",
    "                p.join()\n",
    "            out_data = []\n",
    "            out_labels = []\n",
    "            for data, label in self.out:\n",
    "                if data is not None and label is not None:\n",
    "                    out_data.append(data)\n",
    "                    out_labels.append(label)\n",
    "            if len(out_data) == 0 or len(out_labels) == 0:\n",
    "                continue\n",
    "            if len(np.array(out_data, dtype=object).shape)<4 or len(out_labels) == 0:\n",
    "                continue\n",
    "            yield np.array(out_data), np.array(out_labels)\n",
    "            \n",
    "    def load_files(self, source_folder):\n",
    "        np.random.seed()\n",
    "        data_folder = np.random.choice(source_folder)\n",
    "        speaker = np.random.choice(os.listdir(data_folder))\n",
    "        folder = os.path.join(data_folder, speaker.decode(\"utf8\"))\n",
    "        files = sorted(os.listdir(folder))\n",
    "        if len(files)<=self.out_size:\n",
    "            audio = self.empty_sequence(self.out_size-len(files))\n",
    "            label = [False for _ in range(self.out_size-len(files))]\n",
    "            start_choice = 0\n",
    "        else:\n",
    "            start_choice = np.random.choice(range(len(files)-self.out_size))\n",
    "            audio = []\n",
    "            label = []\n",
    "        for file in files[start_choice:start_choice+self.out_size]:\n",
    "            label.append(str(file).endswith(\"1.wav\"))\n",
    "            wave, sr = librosa.load(os.path.join(folder,file))                        \n",
    "            if len(wave)<=self.speech_length:\n",
    "                pad = [0 for _ in range(self.speech_length - len(wave))]\n",
    "                wave = pad+list(wave)\n",
    "                audio.append(wave)\n",
    "            else:\n",
    "                start_ind = np.random.choice(range(len(wave)-self.speech_length))\n",
    "                features = wave[start_ind:start_ind+self.speech_length]\n",
    "                features = librosa.feature.mfcc(features, sr= sr)\n",
    "                audio.append(features)\n",
    "        try:\n",
    "            audio = np.stack(audio, axis = -1)\n",
    "            self.out.append([audio,True in label])\n",
    "        except:            \n",
    "            self.out.append([None, None])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-queue",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "class LossAndErrorPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with open('CNN_LSTM_bias_1_5.txt','a',encoding = 'utf8') as fw:\n",
    "            fw.write(\"For epoch {}\".format(epoch))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"Loss is       {:7.2f}, val_loss is      {:7.2f}.\".format(logs[\"loss\"],logs[\"val_loss\"]))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"Accuracy is   {:7.2f}, val_accuracy is  {:7.2f}.\".format(logs[\"accuracy\"],logs[\"val_accuracy\"]))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"Recall is     {:7.2f}, Presicion is     {:7.2f}.\".format(logs[\"recall\"],logs[\"precision\"]))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"Val_Recall is {:7.2f}, Val_Presicion is {:7.2f}.\".format(logs[\"val_recall\"],logs[\"val_precision\"]))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"F1 is         {:7.2f}, Val_F1 is        {:7.2f}.\".format(logs[\"f1_score\"],logs[\"val_f1_score\"]))\n",
    "            fw.write(\"\\n\")\n",
    "            fw.write(\"=\"*100) \n",
    "            \n",
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = Precision(thresholds=0.5)\n",
    "        self.recall_fn = Recall(thresholds=0.5)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        # since f1 is a variable, we use assign\n",
    "        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\n",
    "\n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        # we also need to reset the state of the precision and recall objects\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0)            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    data_path = '/home/ubuntu/ProjectVietVu/splited_data'\n",
    "    out_size = 5\n",
    "    speech_len = 1\n",
    "    data_gen = DataGenerator(data_path, out_size, speech_len)\n",
    "    train_gen = data_gen.train_generator(batch_size = 16)\n",
    "    valid_gen = data_gen.valid_generator(batch_size = 8)\n",
    "    print('start')\n",
    "    X, y = next(train_gen)\n",
    "    print(X.shape, y.shape)\n",
    "    base_model = Model(out_size)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"CNN_LSTM_bias_1_5_f1_v2.h5\", monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "    a = []\n",
    "    b = []\n",
    "    print(\"Begin calculate bias weight\")\n",
    "    bt = time.time()\n",
    "    for i in range(100):\n",
    "        X, y = next(train_gen)\n",
    "        a.append(sum(y))\n",
    "        b.append(len(y))\n",
    "    pos = sum(a)\n",
    "    neg = sum(b) - pos\n",
    "    total = sum(b)\n",
    "    print(\"Possitive: %s\"%pos)\n",
    "    print(\"Negative: %s\"%neg)\n",
    "    print(\"In times: %s\" %(time.time() - bt))\n",
    "    weight_for_1 = (1 / pos)*(total)/2.0 \n",
    "    weight_for_0 = (1 / neg)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    initial_bias = np.log([pos/neg])\n",
    "    single_model = base_model.single_lstm(X.shape[1:], initial_bias)\n",
    "    history = single_model.fit(train_gen,\n",
    "                                steps_per_epoch = 100,\n",
    "                                epochs = 20,\n",
    "                                verbose = 1,\n",
    "                                shuffle = False,\n",
    "                                validation_data = valid_gen,\n",
    "                                validation_steps = 20,\n",
    "                                class_weight = class_weight,\n",
    "                                callbacks = [checkpoint,LossAndErrorPrintingCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack([a.T,a.T], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / 1)*(10)/2.0 \n",
    "weight_for_1 = (1 / 9)*(10)/2.0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-discharge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(valid_gen)\n",
    "single_model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 485+240\n",
    "neg = 485\n",
    "pos = 240\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ubuntu/ProjectVietVu/splited_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "a = glob.glob(os.path.join(data_path,\"*/*/*.wav\"))\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folder = sorted(glob.glob(os.path.join(data_path,\"*/*/\")))\n",
    "available = [len(os.listdir(folder))-5 for folder in list_folder if len(os.listdir(folder))>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for folder in list_folder:\n",
    "    files = sorted(os.listdir(folder))\n",
    "    if len(files)<5:\n",
    "        continue\n",
    "    for i in range(len(files)-5):\n",
    "        fs = files[i:i+5]\n",
    "        if any([f.endswith(\"1.wav\") for f in fs]):\n",
    "            pos.append([os.path.join(folder, f) for f in fs])\n",
    "        else:\n",
    "            neg.append([os.path.join(folder, f) for f in fs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[:5], neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos), len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "1304/3394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "for i, f in enumerate(a):\n",
    "    librosa.load(f)\n",
    "    if i%100 == 0:\n",
    "        print(i, time.time() - begin)\n",
    "        begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGenerator(data_path, out_size, speech_len)\n",
    "train_gen = data_gen.train_generator(batch_size = 8)\n",
    "X, y = next(train_gen)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-postage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
